{
  "hash": "bbd90c14bb70e7db219f020a9d8676cf",
  "result": {
    "markdown": "---\ntitle: \"Real Estate Valuation with R: Chapter 1 - Uncertainty\"\nsubtitle: | \n  Get in, loser, we're generating deviates.\ndate: \"2023/01/01\"\nformat: \n  html:\n    fig-width: 8\n    fig-height: 4\n    code-fold: false\nexecute:\n  freeze: auto\n# image: image\ncategories: [real estate]\ndraft: true\n---\n\n\nI'm trying to build some functions that calculate how much a real estate project is worth. I want to re-run those functions a few (thousand) times to explore the impact of the values I think are better viewed as probabilistic. Here, I show my thought process in how I'm approaching these random values.\n\n\n\n\n\n## Introducing the unknowns\n\nIn my commercial real estate project, there are three main categories of values that might vary:\n\n1.  Construction - how long construction and renovation in the building might take\\\n2.  Lease-Up Period - how long we should expect it to take to reach stable vacancy rates\\\n3.  Vacancy Rates - how much stable vacancy we should expect\n\nWe might expect renovations to take 2 years, with a lease-up period of 2 years until we reach a stable office vacancy rate of 15%. What if construction takes longer? What if the market is strong and we fill office leases within one year? What if a pandemic causes stable vacancy to be closer to 30%?\n\nIt would be prudent for any of my models to account for that uncertainty. I don't know enough about real estate or the future make good estimates about how to represent the kind of uncertainty a commercial real estate project would face. Ideally, I would seek the opinions of experts with access to better information, but there's no money on the line here, so I'll make something up that seems close enough.\n\n## Modeling the unknowns: 2 approaches\n\nI will use two different methods for generating random scenarios for the project. The first is using the `sample()` function, which lets me sample outcomes from a discrete set of outcomes and weights. The second is using distribution functions from the `stats` package to generate continuous data within some general parameters.\n\n### Discrete sampling for small sample spaces\n\nFor construction delays and lease-up periods, I am only dealing with small ranges of 0-5 years. To make things simpler, I am only modeling discrete integer outcomes. The accounting won't care about half-years of construction; I will only consider construction \"done\" on the year it finishes.\n\nDiscrete samples can be drawn by giving vectors of outcomes and probability weights to the `sample()` function. This function assumes you are drawing **without replacement** so you will need to set `replace = TRUE` if you are repeating the same drawing multiple times.\n\nBelow is an example for modeling construction times. I estimate construction and renovation to take 2 years, with a roughly a 10% likelihood of a 1-year delay, a roughly 1% likelihood of a 2-year delay, and a 0.1% likelihood of a 3-year delay. We can simulate 50,000 outcomes of this sample like so:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234) # helps me reproduce outcomes predictably\nn = 50000 # number of rows\n\n# store sample draws to dataframe\ndf <- tibble(\n  construction_term=sample(\n    2:5,\n    n,\n    replace=TRUE,\n    prob=c(0.9,0.09,0.009,0.001)\n  )\n)\n```\n:::\n\n\nShowing a histogram of these outcomes is going to show us what we expect: ${prob}*n$ instances of each outcome, more or less.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nfig_colors <- RColorBrewer::brewer.pal(7,'Set1')\n\ndf %>%\n  ggplot(data=.,aes(x=construction_term)) +\n  geom_histogram(\n    bins = length(unique(df$construction_term)),\n    fill=fig_colors[1]\n  ) +\n  ggtitle('Construction Term Frequencies, n=50,000') +\n  xlab('Number of Years') + \n  ylab('Frequency') +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![Simulated outcomes for construction term](index_files/figure-html/hist-construction-1.png){width=768}\n:::\n:::\n\n\nI use the `sample()` function for the construction term as well as the lease-up periods for each of the three types of spaces: retail, office, and residential. Below are the distributions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# store sample draws to dataframe\ndf <- tibble(\n  construction_term=sample(\n    2:5,\n    n,\n    replace=TRUE,\n    prob=c(0.9,0.09,0.009,0.001)\n  ),\n  leaseup_office=sample(\n      1:4, \n      n, \n      replace=TRUE,\n      prob=c(0.05,0.8,0.2,0.05)\n  ),\n  leaseup_residential=sample(\n    2:3, \n    n, \n    replace=TRUE,\n    prob=c(0.8,0.2)\n  ),\n  leaseup_retail=sample(\n    1:4,\n    n,\n    replace=TRUE,\n    prob=c(0.15,0.7,0.10,0.05)\n  )\n)\n```\n:::\n\n\nThis gives us the following distributions in @fig-samples:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndf[1:4] %>% \n  pivot_longer(cols = everything()) %>%\n  ggplot(data=.,aes(x=value)) +\n  geom_histogram(bins = 5) +\n  facet_wrap(~name) +\n  aes(fill=name) +\n  scale_fill_manual(values=fig_colors[1:4]) +\n  guides(fill='none') +\n  ggtitle('Random Variable Frequencies, n=50,000') +\n  xlab('Number of Years') +\n  ylab('Count') +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![Simulated outcomes for four `sample()` columns](index_files/figure-html/fig-samples-1.png){#fig-samples width=768}\n:::\n:::\n\n\nThis approach of sampling from a small set of outcomes works well when there are a few set outcomes possible. But what about a larger set of outcomes? What about continuous random variables?\n\n### Continuous sampling\n\nThe `stats` package in R contains many functions to handle distributions, including **normal**, **Poisson**, **uniform**, and **binomial** distributions. I want to use these to model potential outcomes for the three vacancy random variables: `vacancy_retail`, `vacancy_office`, and `vacancy_residential`. The retail and office vacancy RVs have the risk of being higher than we hope; there may be a recession or a pandemic that causes low occupancy for shops or offices. For these two values, I use a a **beta distribution** to model a right-skewed outcome space. Because there is a lack of housing in the downtown area, vacancy rates for apartments are likely to fluctuate less and be overall lower, so I use a **normal distribution** for those outcomes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- df %>%\n  # add new columns\n  mutate(\n    vacancy_retail = 0.15 + rbeta(n,2.0,10),\n    vacancy_office = 0.15 + rbeta(n,1.2,10),\n    vacancy_residential = rnorm(n,0.07,0.03),\n    # fix negative vacancies\n    vacancy_residential = ifelse( \n      vacancy_residential<0,\n      0,\n      vacancy_residential\n    )\n  )\n```\n:::\n\n\nThese distributions aim to capture the following sentiments: the residential market will be very hot, the office market will be lukewarm, and the retail market will be slightly cooler still. The frequency of outcomes for these distributions is shown in @fig-distributions. The `rbeta()` function generates random deviates of a custom [beta distributions](https://en.wikipedia.org/wiki/Beta_distribution). I haven't been exposed to the type of statistics that would make me familiar with beta distributions; I just searched types of random distributions that fit my version of a highly right-skewed distribution. I think this matches my vision of the small chance of high vacancy rates above some baseline rate. I use the `rnorm()` function for residential vacancy rates since I am very confident those rates will be low and will not deviate strongly. \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndf %>% \n  select(starts_with('vacancy')) %>%\n  pivot_longer(cols=everything(), names_to='Floor Type') %>%\n  ggplot(\n    data=.,\n    aes(x=value,linetype=`Floor Type`),\n  ) +\n  geom_freqpoly(bins=40) +\n  scale_x_continuous(labels=scales::label_percent()) +\n  ggtitle('Continuous RV Frequencies, n=50,000') +\n  xlab('Vacancy Rate') +\n  ylab('Count') +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![Simulated outcomes for three distribution columns](index_files/figure-html/fig-distributions-1.png){#fig-distributions width=768}\n:::\n:::\n\n\n## Concave, Convex\n\nIt seems like a lot of work to estimate thousands of potential outcomes for these seven variables. We could always calculate the average  values for our variables and just use those once in our model. In @tbl-means, we see those values wouldn't be hard to use. \n\n\n::: {#tbl-means .cell tbl-cap='Mean values for random variables'}\n\n```{.r .cell-code  code-fold=\"true\"}\ndf %>%\n  summarize(across(everything(),mean)) %>%\n  t() %>%\n  kable()\n```\n\n::: {.cell-output-display}\n|                    |          |\n|:-------------------|---------:|\n|construction_term   | 2.1123000|\n|leaseup_office      | 2.2211000|\n|leaseup_residential | 2.2001000|\n|leaseup_retail      | 2.0500600|\n|vacancy_retail      | 0.3168757|\n|vacancy_office      | 0.2578244|\n|vacancy_residential | 0.0698093|\n:::\n:::\n\n\nThis misses a critical point about Monte Carlo simulation: the output of the average of all inputs might not be the same as the average outputs of each possible input.  MC simulation produces many possible input values, with the likelier values occurring more often in the generated data. If we just take the average expected value of our input and apply the function $u$ to just that value, we may end up with a biased estimate. That bias depends on whether the function $u$ is concave or convex. Lets return to dice to explain. Suppose we roll a standard die and apply a function to the result. We want to estimate the expected result of each function for any random roll of the die.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Dice Simulation\ndice = 1:6 # outcomes for dice\nn = 10000 # number of samples\n\nset.seed(4321) # makes this reproducible\n\nrolls = sample(\n  dice, #samples from dice\n  n, # takes n samples\n  replace=TRUE # samples with replacement\n)\n```\n:::\n\n\nIn the chunk above, I've rolled a 6-sided die 10,000 times and recorded each outcome to the vector `rolls`. The functions I'll apply to these rolls are:\n\n* $ f(x) = x^2 $, a quadratic (convex) function\n* $ g(x) = 2x $, a linear function, and\n* $ h(x) = ln(x) $, a logarithmic (concave) function.\n\nIf we want the easy way out, we can hope that $Y=u(E(X))$ and substitute $u$ for our functions above. Remember, in the case of dice rolls, $E(x)=3.5$. That would give us:\n\n* $ E(f(x)) \\stackrel{?}{=} f(E(x)) = f(3.5) = 3.5^2 = 12.25 $\n* $ E(g(x)) \\stackrel{?}{=} g(E(x)) = f(3.5) = 2*3.5 = 7 $\n* $ E(h(x)) \\stackrel{?}{=} h(E(x)) = f(3.5) = ln(3.5) \\approx 1.25276 $\n\nOur generating function for $x$ has some random error: our average roll is 3.4963, not 3.5. Applying our functions to both this expected value and our vector of simulated outcomes gives us results shown in @tbl-sim. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define rolls\nrolls <- 1:6\n\n# define functions\nf <- function(x){x^2}\ng <- function(x){2*x}\nh <- function(x)(log(x))\n\n# store expected value of x\nE_x <- mean(rolls)\n```\n:::\n\n::: {#tbl-sim .cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# display table\ntibble(\n  'Function'=c('quadratic (convex)','linear','logarithmic (concave)'),\n  'E(x)'=E_x,\n  Predicted=c(f(E_x),g(E_x),h(E_x)),\n  Actual=apply(data.frame(f(rolls),g(rolls),h(rolls)), MARGIN=2,FUN=mean),\n  Error=((Actual-Predicted)/Actual) %>% scales::label_percent()()\n) %>% \n  rename(\n    'Predicted u(E(x))'=Predicted,\n    'Actual E(u(x))'=Actual\n  ) %>%\n  kable(digits=2)\n```\n\n::: {.cell-output-display}\n|Function              | E(x)| Predicted u(E(x))| Actual E(u(x))|Error |\n|:---------------------|----:|-----------------:|--------------:|:-----|\n|quadratic (convex)    |  3.5|             12.25|          15.17|19%   |\n|linear                |  3.5|              7.00|           7.00|0%    |\n|logarithmic (concave) |  3.5|              1.25|           1.10|-14%  |\n:::\n:::\n\n\nNow we can see that simply plugging in our average value for the input does not always translate to a good estimate for the output of our function. If we are modeling $Y=u(X)$ for many possible $X$ values, we must remember:\n\n\n$$ E(Y) \\approx E(u(X)) \\neq u(E(X)) $$\n\n\nThis is known as [Jensen's Inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality). For my real estate project, I don't know the nature of the relationship between my seven random input variables and my output variable. The pro forma might ultimately be close enough to linear that this whole simulation approach ends up mostly redundant. If so, that's a valuable finding! If not, I'll know how far and in what direction a typical pro forma estimate would be biased without simulation.\n\n## Conclusion\n\nI have defined the seven random variables that will serve as input data for my real estate valuation project. Up next, a very simple pro forma and a look inside the time complexity considerations of a non-programmer.\n\n## Further Reading\n\nA neat overview of techniques in group decision making and forecasting can be found [here](https://doi.org/10.1093/acrefore/9780190236557.013.262).\n\nI spent an hour trying to understand beta distributions from Wikipedia [here](https://en.wikipedia.org/wiki/Beta_distribution). I will keep this tab open on my phone until I understand it or until I die, whichever comes first.\n\nA detailed exploration of random distributions in Microsoft Excel can be found [here](https://cameron.econ.ucdavis.edu/excel/ex23normaltprobabilities.html) (though do consider whether you should still be in a spreadsheet at this point).\n\nSome great refreshers of random distributions in R can be found [here](https://rstudio-pubs-static.s3.amazonaws.com/100906_8e3a32dd11c14b839468db756cee7400.html) and [here](https://www.huber.embl.de/users/kaspar/biostat_2021/2-demo.html).\n\nThe R `stats` package documentation on distributions is [here](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/Distributions.html).\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}