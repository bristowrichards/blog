[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I feel compelled to read Wikipedia until I know everything. Please send me your favorite niche articles!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "quarto-blog",
    "section": "",
    "text": "Learning about ggplot with chess\n\n\nI’m discovering some nuances about ggplot and want to explore it using chess ratings data\n\n\n\n\nviz\n\n\nchess\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2023\n\n\nBristow Richards\n\n\n\n\n\n\n  \n\n\n\n\nReal Estate Valuation with R: Introduction\n\n\nI want to practice simulation and optimization in R. I’ll use real estate to do so.\n\n\n\n\nreal estate\n\n\nsimulation\n\n\n\n\n\n\n\n\n\n\n\nDec 17, 2022\n\n\nBristow Richards\n\n\n\n\n\n\n  \n\n\n\n\nCreating a Project Network Diagram\n\n\nA messy workflow for inheriting a messy data project\n\n\n\n\nproject management\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\nBristow Richards\n\n\n\n\n\n\n  \n\n\n\n\nWelcome!\n\n\nI’m starting a blog. Mediocre data takes to come.\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2022\n\n\nBristow Richards\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Bristow Richards",
    "section": "",
    "text": "Copyright (c) 2022 Bristow Richards\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "posts/diagram1/index.html",
    "href": "posts/diagram1/index.html",
    "title": "Creating a Project Network Diagram",
    "section": "",
    "text": "I was working on a data project that was at once meticulously organized and impossibly messy. There were no problems with the code, the repository structure, or the documentation. The problem was knowledge transfer. The author of the project had left my organization before I began, and no one had really looked over his code to ask him questions before he left.\nThe project combined many independent files from sources that required users to manually pull data, and the files would often have irregular formatting or other government-produced spreadsheet idiosyncrasies that made it impossible to fully automate the data acquisition process. Still, the project housed scripts that automated everything else the data sources needed to be combined, making the final product itself as reproducible as possible.\nI was tasked with better documenting the dozens of files, scripts, and utilities so that staff could more easily maintain this repository going forward. I had very little background in the subject matter of the data project and had no idea where to begin. I started scribbling down the names of scripts on a piece of paper and drawing lines between them. It wasn’t long before I looked like a conspiracy theorist at the central of a devious plot of Census data and HUD variable refactoring. I knew there had to be a better way."
  },
  {
    "objectID": "posts/diagram1/index.html#enter-graphs",
    "href": "posts/diagram1/index.html#enter-graphs",
    "title": "Creating a Project Network Diagram",
    "section": "Enter: graphs",
    "text": "Enter: graphs\nI decided to try using a network diagram rendered from a graph. In most data projects, “graph” means “mapping data to aesthetics to visually represent data features” like a barplot or scatterplot. No, I am not using ggplot today. Rather, I am going to use the discrete math definition of a graph, in the sense of representing the relationship between unique entities (read more).\nIn a graph, you define “nodes” (entities of interest to you) and “edges” (their relationships). It looks like Figure 1:\n\n\n\n\n\nFigure 1: A simple graph\n\n\n\nThis figure represents a graph. It shows that there exist two nodes A and B, and that there exists a single edge relating the two. This image says: “A and B are related.”\nThis graph is not helpful to me, because my network of datasets and scripts does have directed relationships. I can’t just say “script 3 and dataset 3 are related.” I need to say “script 3 creates dataset 3.” This is fine, though, because edges can have directions if you specify. Something like Figure 2 would represent my issue.\n\n\n\n\n\nFigure 2: A simple directed graph\n\n\n\nThis figure seems closer to what I want to accomplish in documenting my complicated project. The directed edge could mean a number of things. Here, I meant for it to say “script 3 creates dataset 3,” though this is not always what an edge means. In a large messy repository of many scripts and intermediate utilities, scripts might have many inputs to represent. For example, in part of my project, there was a script which pulled in source data and modified it according to a dictionary .json object which had been produced by another project and which (importantly!) would never be modified. This script combined the source data and the dictionary to produce clean data. I call most input objects “utils” if they are not regular tabulated data. This case is represented in Figure 3.\n\n\n\n\n\nFigure 3: A more complex directed graph\n\n\n\nNow, it is more apparent that a directed edge is probably best translated to “informs” or “is used in.” The above image says “there exists a script which is informed by source and dict_util and which informs a dataset called clean_data.” This is the kind of image which I want to represent my repository so I can track how everything relates.\nTo be pedantic, these images are not the graphs. Graphs are abstract math models for conceiving relationships. These images are rendered representations of graphs. It would be easy for someone to read Figure 1 and say A is “above” or otherwise related in a hierarchy to B. That could have been my intention, but in this instance it was an arbitrary choice of my program when it decided to render the graph object \"A--B\".\nThis divorce between abstract relationships and visual representations is an important one. You can create graphs and investigate relational properties between nodes without ever rendering the graph visually. Rendering graphs visually is, of course, important for communicating relationships and meaning to others. I could feed the graph from Figure 3 into a program and, instead of rendering it, perform actions like “show me all antecedents to script” or “show me how many nodes separate source and clean_data from each other.” For more reading about graph data science, I highly recommend this blog post in which Tomaz Bratanic uses graphs to represent and analyze streamers on the Twitch platform.\nIn the end, though, I’m not a graph data scientist. I’m a busy intern who needs to document a repository. So, I will now cease using any distinction between a graph and its visual representation, and I will get back to the project at hand: making a graph diagram."
  },
  {
    "objectID": "posts/diagram1/index.html#the-diagrammer-package",
    "href": "posts/diagram1/index.html#the-diagrammer-package",
    "title": "Creating a Project Network Diagram",
    "section": "The DiagrammeR package",
    "text": "The DiagrammeR package\nWe need to create an informative visual representation of a network which can probably be modeled well using a directed graph. Like can be said for most things, there is an R package that can help us with this niche problem. The DiagrammeR package handles a number of analytic tasks related to graphs, including constructing graphs from dataframes of nodes and edges, querying graphs for relationships between nodes, and visually representing graphs in diagrams like the figures above. My main focus being the visual representation of my graph, I used the grViz() function to feed a Graphviz handler my graph data in DOT notation. I didn’t understand what that sentence meant two months ago, and I barely understand what it means now, but it goes something like this:\n\ngrViz(\"\ndigraph {\n  node [shape = circle,\n        fontname = Helvetica,\n        fontsize = 12]\n        \n  dict_util->script\n  source->script\n  script->clean_data\n}\n\")\n\n\n\n\nFigure 4: A more complex directed graph, with code\n\n\n\nIn this snippet above, I make three declarations in the form of source->script to define edges, which implicitly defines the existence of the nodes source and script. I then define general node attributes like shape and font. These are combined in either a graph {} or digraph {} call, depending on whether the graph is undirected or directed (in this case, it is directed, so I use digraph {}). Finally, all of this is then gathered as a large string and passed to grViz().\nI want to do a little bit more with this. I want to group nodes by color so I can quickly tell which nodes are scripts and which nodes are datasets. Also, I want source datasets to have folder-shaped nodes. This will be helpful later, because some data comes from our organization’s SharePoint and some comes directly from links, and I’d like to distinguish between them. The code below which produces Figure 5 shows how I would manually create such a figure. As I add more details to the aesthetic rendering of the nodes, I start referring to nodes numerically as tab1, tab2, etc. This naming is arbitrary but will help later when I want to handle more nodes. It helps a bit with readability, too, because the node names are all similar in length now.\n\ngrViz(\"\ndigraph {\n  node [fontname = Helvetica, style = filled]\n  \n  tab1 [label = 'source', shape = folder, fillcolor = green]\n  tab2 [label = 'util', shape = rectangle, fillcolor = purple]\n  tab3 [label = 'script', shape = rectangle, fillcolor = grey]\n  tab4 [label = 'output', shape = rectangle, fillcolor = green]\n  \n  tab1 -> tab3\n  tab2 -> tab3\n  tab3 -> tab4\n}\n\")\n\n\n\n\nFigure 5: A colorful digraph, with code\n\n\n\nThe code here is a little awkward and seems like it could have been simplified. It could! But that simplification would only matter if I were manually creating the string argument for the grViz() function. I intend rather to use tabular node and edge data to create a simple string for each node and each edge, then combine these string values into the larger string which grViz() requires. This is definitely the point where I start building up technological debt and the maintenance of my code becomes more difficult than is probably necessary. But! It works."
  },
  {
    "objectID": "posts/diagram1/index.html#the-workflow",
    "href": "posts/diagram1/index.html#the-workflow",
    "title": "Creating a Project Network Diagram",
    "section": "The workflow",
    "text": "The workflow\nA wiser version of myself would have investigated the DiagrammeR documentation fully so that I would know how to turn tabular data on nodes and edges into a graph object, then render that graph object by turning some of the node attributes into aesthetic mappings such as color, size, position, and shape. I did no such thing. Instead, after getting a little confused on how to construct and handle diagram objects, I settled for using the Graphviz method which I just demonstrated. The grViz() function takes as its argument a giant string, which declares nodes, edges, and aesthetics. I wanted to be able to store the node and edge data in a separate file, then use this data to construct the string for grViz(), which felt like cheating a bit and led to some bogus-looking code. I decided on the following workflow:\n\ndocument the objects in the repository\ndocument the relationship between these objects\npull this data into R\nadd aesthetic mappings onto the data\nturn that data into a DOT notation string\npass this string to grViz() and export the resulting figure"
  },
  {
    "objectID": "posts/diagram1/index.html#a-simple-example",
    "href": "posts/diagram1/index.html#a-simple-example",
    "title": "Creating a Project Network Diagram",
    "section": "A simple example",
    "text": "A simple example\nIn this example, I do steps 1-4 as short code statements. For my larger project, I ended up using a larger Microsoft Excel document with a worksheet for “nodes” and another for “edges.” Below, I create small dataframe objects for nodes and edges, then I add two aesthetic mappings as new columns onto the nodes dataframe.\n\nlibrary(DiagrammeR)\nlibrary(tidyverse)\n\n# input node data\nnodesdf <- data.frame(\n  id = 1:4,\n  name = c('source', 'util', 'script', 'output'),\n  category = c('data', 'util', 'script', 'data')\n)\n\n# input edge data\nedgesdf <- data.frame(\n  from = c(1, 2, 3),\n  to = c(3, 3, 4)\n)\n\n# edit node data to map category to color and name to shape\nnodesdf <- nodesdf %>%\n  mutate(\n    color = case_when(\n      category == 'data' ~ 'green', # data nodes will be green\n      category == 'script' ~ 'grey', # script nodes are grey\n      TRUE ~ 'purple' # anything else will be purple\n    ),\n    shape = case_when(\n      name == 'source' ~ 'folder', # source files will be folder-shaped\n      TRUE ~ 'rectangle' # anything else is a rectangle\n    )\n  )\n\nprint(nodesdf)\n\n  id   name category  color     shape\n1  1 source     data  green    folder\n2  2   util     util purple rectangle\n3  3 script   script   grey rectangle\n4  4 output     data  green rectangle\n\nprint(edgesdf)\n\n  from to\n1    1  3\n2    2  3\n3    3  4\n\n\nNext, I will add a new column onto nodesdf to create the necessary string that defines each node’s name, color, and shape. It’s a very ugly use case of a paste() function. Because the eventual argument fed to grViz() is a string, this string which contains strings must use many escape characters to make sure it constructs correctly. I will also add a new column to the edgesdf in a similar fashion.\n\nnodesdf <- nodesdf %>%\n  mutate(\n    node_label = paste0(\n      'tab'\n      ,id,\n      ' [label = \\'',\n      name, # label str\n      '\\', shape = ',\n      shape,\n      ', fillcolor = ',\n      color,\n      ']'\n    )\n  )\n\nprint(nodesdf)\n\n  id   name category  color     shape\n1  1 source     data  green    folder\n2  2   util     util purple rectangle\n3  3 script   script   grey rectangle\n4  4 output     data  green rectangle\n                                                     node_label\n1    tab1 [label = 'source', shape = folder, fillcolor = green]\n2  tab2 [label = 'util', shape = rectangle, fillcolor = purple]\n3  tab3 [label = 'script', shape = rectangle, fillcolor = grey]\n4 tab4 [label = 'output', shape = rectangle, fillcolor = green]\n\nedgesdf <- edgesdf %>%\n  mutate(\n    edge_label = paste0(\n      'tab',\n      from,\n      ' -> tab',\n      to\n    )\n  )\n\nprint(edgesdf)\n\n  from to   edge_label\n1    1  3 tab1 -> tab3\n2    2  3 tab2 -> tab3\n3    3  4 tab3 -> tab4\n\n\nNow having constructed the strings which define the nodes and edges, I finalize the portions of the grViz() arguments which come from node and edge. To do this, I use summarize() to collapse many rows into one, using a paste() function as the summary function. It is important to note that the grViz() function expects a new line or some other delimiter between declarations of nodes and edges. For simplicity’s sake, I use a newline character. Important to note: using the print() and the writeLines() functions will result in different handling of newline characters.\n\n# define nodes string\nnodestr <- nodesdf %>%\n  summarize(paste(node_label, collapse = '\\n')) %>%\n  as.character() # we want a string, not a 1x1 dataframe holding a string\n\n# define edges string\nedgestr <- edgesdf %>%\n  summarize(paste(edge_label, collapse = '\\n')) %>%\n  as.character()\n\n# debugging example\nwriteLines(edgestr)\n\ntab1 -> tab3\ntab2 -> tab3\ntab3 -> tab4\n\n\nFinally, we can combine the two string portions we’ve built into the final string argument for grViz(). I write this to an intermediate object graphstr which I can then pass to the rendering function.\n\n# include some parameters then pass string literals\ngraphstr = paste(\n  \"digraph {\n  node [fontname = Helvetica, style = filled]\",\n  nodestr,\n  edgestr,\n  '}',\n  sep = '\\n'\n)\n\ngrViz(graphstr)\n\n\n\n\n\nWe did it! We found a pretty wonky workflow to turn tabular node and edge data into a graph, including turning node attributes into visual aesthetics for the rendered graph. I am certain there is a simpler way to do this, but I found this problem easier to solve when I knew what I wanted the grViz() call to look like and reverse-engineered that string from my data."
  },
  {
    "objectID": "posts/diagram1/index.html#documenting-a-real-world-example",
    "href": "posts/diagram1/index.html#documenting-a-real-world-example",
    "title": "Creating a Project Network Diagram",
    "section": "Documenting: a real-world example",
    "text": "Documenting: a real-world example\nFor my project, I documented anything that seemed like an important “moving part.” This included any source data links, each script, any important packages, any intermediate datasets (that a project user would have saved to their machine as part of the steps of running each script in order), and any other important utilities. This also included two important “temporary” objects that existed only temporarily within a large script. These temporary datasets were methodologically complex to create and were very sensitive to changes in inputs, so they seemed to merit defining as separate objects that “informed” the script that contained them. I’m not sure how consistent that categorization is, but it was useful at the time.\nI stored my project in an Excel worksheet. The particularities of my project required me to document the “stage” and “substage” of each portion of the project, and also to note the location and path of each item. I added the location to the path1 column and the actual path or link to the item in the path2 column. This naming scheme is bad, but it worked fine for my project. I mostly had the paths and links so I could use the spreadsheet for future reference. Collecting this data on my project took some time to make sure I had written down every item. The only important thing to remember is to give each row an id for reference.\n\n\n\nImage 1: Node table\n\n\nI then documented the binary “from-to” relationships of each node in a new sheet. This sheet had four columns, but the only two that matter are the from and to columns, because the other two are filled with a quick =XLOOKUP() call so that I as a user could better track what I was doing.\n\n\n\nImage 2: Edge table\n\n\nThis spreadsheet served well to document the nodes and edges of my project. You may prefer using .csv files so their changes can be tracked more closely with git, but I found the ease of Excel preferable."
  },
  {
    "objectID": "posts/diagram1/index.html#rendering-my-real-life-example",
    "href": "posts/diagram1/index.html#rendering-my-real-life-example",
    "title": "Creating a Project Network Diagram",
    "section": "Rendering my real-life example",
    "text": "Rendering my real-life example\nHaving source data to pull from, I can now create a diagram of my project. I pull the data from my Excel document using readxl::readxlsx. Then I mutate the nodes dataframe to add aesthetic mappings. This mutate also includes a paste() call to construct the string for each individual node that defines the title, color, and shape of each node. Unique in this case is my use of two attributes for the node name, separated by a newline character. I use each row’s substage and name values to create an entity’s node name. I then construct the graph string argument by collapsing the node and edge dataframes using summarize() then combining them with the other portions of the string value which grViz() expects. I then render the image. This chunk has some commented-out examples of how you could render a high-quality version of this image and save it to .pdf and .png using a combination of DiagrammeRsvg and rsvg.\n\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(DiagrammeR)\nlibrary(DiagrammeRsvg) # exporting to svg improves output resolution\nlibrary(rsvg) # helps handle svg\n\n# file\nxl <- \"data/diagram_data.xlsx\"\n\n# import graph relationship data\nnodesdf <- read_xlsx(xl, sheet = 'Nodes')\nedgesdf <- read_xlsx(xl, sheet = 'Edges')\n\n# add additional node attributes to initial node df\nnodesdf <- nodesdf %>%\n  mutate(\n    # adding shapes as a case of the file 'type'\n    shape = case_when(\n      type == 'script' ~ 'rectangle',\n      type %in% c('prepped', 'final') ~ 'cylinder',\n      type %in% c('util', 'intermediate', 'temp') ~ 'oval',\n      type %in% c('source', 'link') ~ 'folder'\n    ),\n    # adding colors based on type and origin\n    color = case_when(\n      type == 'script' ~ 'white',\n      type == 'temp' ~ 'yellow',\n      type == 'prepped' ~ 'grey',\n      type == 'final' ~ 'greenyellow',\n      type == 'link' ~ 'beige',\n      path1 == 'sharepoint' ~ 'cyan',\n      type == 'intermediate' ~ 'salmon',\n      TRUE ~ 'plum' # this applies to intermediate utilities, mainly\n    ),\n    # define the labels\n    node_label = paste0(\n      'tab',\n      id,\n      ' [label = \\'',\n      substage,\n      '\\n',\n      name,\n      '\\', shape = ',\n      shape,\n      ', fillcolor = ',\n      color,\n      ']'\n    ),\n  )\n\n# create edge label string for each row\nedgesdf <- edgesdf %>%\n  mutate(\n    edge_label = paste0(\n      'tab',\n      from,\n      ' -> tab',\n      to\n    )\n  )\n\n# define nodes string\n# need \\n characters for grViz()\nnodestr <- nodesdf %>%\n  summarize(paste(node_label, collapse = '\\n')) %>%\n  as.character()\n\n# define edges string (similar setup)\nedgestr <- edgesdf %>%\n  summarize(paste(edge_label, collapse = '\\n')) %>%\n  as.character()\n\n# include some parameters then pass string literals\ngraphstr = paste(\n  \"digraph flowchart {\n  node [fontname = Helvetica, style = filled]\",\n  nodestr,\n  edgestr,\n  '}',\n  sep = '\\n'\n)\n\n# for debugging\n# writeLines(graphstr)\n\n# render in R session\ngrViz(graphstr)\n\n\n\n\n\nThe DiagrammeR package doesn’t support “legends” for rendered graph images. This is no problem; because my shapes and colors are coming from just a few columns, I can make a second dataframe drawing only from those important columns, then select only unique rows using distinct(). I can then use nearly the same workflow as above to produce a legend that visually shows what each shape and color combination represent.\n\n# create legend df for unique color/shape combos\nlegend <- nodesdf %>%\n  distinct(color, shape, type, path1) %>%\n  mutate(\n    id = row_number(),\n    legend_label = paste0(\n      'tab',\n      id,\n      ' [label = \\'',\n      'type: ',\n      type,\n      '\\n',\n      'path: ',\n      path1,\n      '\\', shape = ',\n      shape,\n      ', fillcolor = ',\n      color,\n      ']'\n    )\n  )\n\n# collapse into grviz-compatable string\nlegendstr <- legend %>%\n  summarize(paste(legend_label, collapse = '\\n')) %>%\n  as.character()\n\n# create grviz string\nlegendgraphstr = paste(\n  \"digraph {\n  node [fontname = Helvetica, style = filled]\",\n  legendstr,\n  ';}',\n  sep = '\\n'\n)\n\n# for debugging\n# writeLines(legendgraphstr)\n\n# render in RStudio viewer\ngrViz(legendgraphstr)\n\n\n\n\n\nNote that, in the above renderings, there are tooltips for each node titled “node1” “node2” etc."
  },
  {
    "objectID": "posts/diagram1/index.html#exporting-diagrams",
    "href": "posts/diagram1/index.html#exporting-diagrams",
    "title": "Creating a Project Network Diagram",
    "section": "Exporting Diagrams",
    "text": "Exporting Diagrams\nThe DiagrammeR package sometimes struggles to export high-quality images outside of the rendering that occurs in your RStudio session. Using the DiagrammeRsvg and rsvg packages, you can save high quality diagram renderings to many formats. First, render the diagram with grViz(), then use export_svg() to turn that image to an svg object, then convert the svg string to raw data with charToRaw(), then save the object with an rsvg function. Below is an example of exporting both the diagram and a legend to both .pdf and .png formats.\n\n# export to high quality pdf from svg\ngrViz(graphstr) %>%\n  export_svg() %>% charToRaw() %>% rsvg_pdf(\"output/diagram.pdf\")\n\n# export to high quality png from svg\ngrViz(graphstr) %>%\n  export_svg() %>% charToRaw() %>% rsvg_png(\"output/diagram.png\")\n\n# export to high quality pdf from svg\ngrViz(legendgraphstr) %>%\n  export_svg %>% charToRaw %>%\n  rsvg_pdf(\"output/diagram-legend.pdf\")\n\n# export to high quality png from svg\ngrViz(legendgraphstr) %>%\n  export_svg %>% charToRaw %>%\n  rsvg_png(\"output/diagram-legend.png\")\n\nAnd here is the high-quality .png output of the main graph diagram:"
  },
  {
    "objectID": "posts/diagram1/index.html#conclusion",
    "href": "posts/diagram1/index.html#conclusion",
    "title": "Creating a Project Network Diagram",
    "section": "Conclusion",
    "text": "Conclusion\nRendering a project in a directed graph like this was very helpful for me. I understood how scripts worked together. I had a better idea what “downstream” scripts needed a spot check when a data source updated. I knew which portions of the repository I could leave alone and which parts would be more “sensitive” to changes in variable names, years, and formats.\nI don’t think the code I wrote is very pretty. I think it would be a cleaner and more flexible approach to use my input data to create a graph object with DiagrammeR, then use functions apart from Graphviz to render the graphs. My weird string-creating functions serve as an exercize in technological debt; they aren’t immediately intuitive to other users, and maintaining them would have small marginal returns compared to just using the normal node aesthetic functions DiagrammeR provides. Nevertheless, I technically succeeded in my goal of flexibly rendering this one project’s repository. That was, after all, my goal.\nSo far, I have not been able to find other projects where R users create images in DiagrammeR without manually typing everything in a Graphviz statement. My goal is to avoid that and rather make the diagram part somewhat automatic so people like me can focus on the documentation of our projects.\nI am no expert in graphs, I just think they’re neat. I don’t understand them very well, but discrete math topics like topology and graph theory fascinate me and make me curious about what tools I now lack which could make my work easier and more conceptually clear.\nIf, one day, I refine this workflow better, I will try to share the R project which defines the necessary functions to render a project."
  },
  {
    "objectID": "posts/diagram1/index.html#further-reading",
    "href": "posts/diagram1/index.html#further-reading",
    "title": "Creating a Project Network Diagram",
    "section": "Further reading",
    "text": "Further reading\nDocumentation for DiagrammeR can be found here.\nSpecific information on Graphviz functionalities in DiagrammeR can be found here.\nThe blog I linked above which discusses using data science to analyze Twitch streamers is here.\nA very neat blog about using DiagrammeR which discusses more complicated graphs (including “subgraphs”) can be found here.\nThe source code for this blog can be found at my GitHub."
  },
  {
    "objectID": "posts/real_estate_0/index.html",
    "href": "posts/real_estate_0/index.html",
    "title": "Real Estate Valuation with R: Introduction",
    "section": "",
    "text": "This semester, I took a Real Estate Development class because I thought I would learn about affordable housing. I didn’t learn about affordable housing, but I did learn how capitalists think about land. I’m not an accountant and don’t have much of an interest in the world of commercial real estate valuation, but I enjoyed the final project: whether to purchase a for-sale office building, and what to do with it."
  },
  {
    "objectID": "posts/real_estate_0/index.html#scoping-the-problem",
    "href": "posts/real_estate_0/index.html#scoping-the-problem",
    "title": "Real Estate Valuation with R: Introduction",
    "section": "Scoping the Problem",
    "text": "Scoping the Problem\nModeling the problem was not hard. The question at hand is: how much money can you make? We had soft constraints too, such as a desire to have affordable housing in the downtown Pittsburgh neighborhood and a desire to maintain the historic facade of the building to contribute a beautiful aesthetic to the neighborhood. That second part was a lie, we wanted a historic tax credit for money.\n\n\n\nDowntown Pittsburgh - Matt Evans, Flickr\n\n\nThis could be a large optimization problem: maximize returns / minimize cost given certain parameters and certain integer decision making variables. Apartments make a certain amount per unit, but also the decision to have a single apartment brings about fixed costs, so it might be best to do zero units or 60 units instead of something in between."
  },
  {
    "objectID": "posts/real_estate_0/index.html#the-modeling",
    "href": "posts/real_estate_0/index.html#the-modeling",
    "title": "Real Estate Valuation with R: Introduction",
    "section": "The modeling",
    "text": "The modeling\nThe whole thing could be modeled easily in a spreadsheet. I love to hate spreadsheets for lack of reproducibility and transparency of calculations, but it went really well. We had two sheets: a pro forma of the costs, revenue, stable operating income, and value at the end of the project. We had a second sheet with all of our parameters: size of new apartments, lease per square foot of retail, interest rate for our loan, and the different vacancy rates for different types of floors, to give some examples.\nNot all of these parameters are known. Some are decisions: we can set apartment prices, we can allocate more or less floor space to retail, and we know what our set land tax rates will be. However, many more parameters are unknown and must be estimates. We used fairly arbitrary numbers: 15% vacancy for retail, 7.5% exit capitalization rate, and a 1-year office fill-out period after renovation. However, these might be better understood as probabilistic values. Our vacancy for retail is estimated to be 15%. What if there’s a recession and we have a 50% vacancy? What if market dynamics change and we have to use an exit cap rate of 9.5%?"
  },
  {
    "objectID": "posts/real_estate_0/index.html#the-approach-monte-carlo-simulation",
    "href": "posts/real_estate_0/index.html#the-approach-monte-carlo-simulation",
    "title": "Real Estate Valuation with R: Introduction",
    "section": "The Approach: Monte Carlo Simulation",
    "text": "The Approach: Monte Carlo Simulation\nApproaching an outcome based on a randomly distributed input is what simulation is for. If we have some output \\(Y\\) that is a function \\(Y=u(X)\\), we can calculated the expected value of \\(Y\\) by calculating:\n\\[E(Y)=E(u(X))\\]\nBy randomly simulating many instances of \\(X\\) (each instance ) in its distribution, we can simple use averages to find the expected value of \\(X\\). Selecting \\(n\\) instances of \\(X\\), each titled \\(x_i\\) for \\(i \\in \\{1,2,3,...n\\}\\), we estimate:\n\\[ \\hat\\mu = \\frac {u(x_1) + ... + u(x_n)} {n} \\approx E(u(x)) = E(Y) \\]\nSo if you have a well-defined relationship between all your randomly-distributed inputs and your output, it can be easier to replicate an instance of that function thousands of times instead of very precisely defining the relationship between the distributions of each input."
  },
  {
    "objectID": "posts/real_estate_0/index.html#an-example-dice",
    "href": "posts/real_estate_0/index.html#an-example-dice",
    "title": "Real Estate Valuation with R: Introduction",
    "section": "An example: dice",
    "text": "An example: dice\nIf you want to know the expected value of rolling a dice, you could just roll a dice \\(n\\) times, summing the value for each roll \\(x_i\\) and dividing by \\(n\\) to get the final value (which should amount to 3.5).\n\n# Dice Simulation\ndice = 1:6 # outcomes for dice\nn = 10000 # number of samples\n\nset.seed(8675309) # makes this reproducible\n\nrolls = sample(\n  dice, #samples from dice\n  n, # takes n samples\n  replace=TRUE # samples with replacement\n)\n\nNow we have a list rolls which contains 10,000 outcomes of rolling a 6-sided die. We can display a histogram of outcomes to see how likely each outcome appears to have been:\n\n\nCode\nlibrary(ggplot2)\n\nggplot() + \n  aes(rolls) + \n  geom_histogram(\n    color='black',\n    bins=6\n  ) + \n  ggtitle('Dice Roll Outcomes') +\n  xlab('Roll Value') +\n  ylab('Frequency')\n\n\n\n\n\nFigure 1: Outcome of 10,000 Dice Rolls\n\n\n\n\nEverything looks in order. We rolled the die 10,000 times, and each outcome appears to have happened. We can take the average outcome for rolls to estimate the expected outcome of rolling a dice:\n\n# calculate mean outcome\nmean(rolls)\n\n[1] 3.4998\n\n\n\n\n\nThat’s pretty close! We got a value of 3.4998, which is only off by 0.0057%. Calculating the expected value of many dice rolls isn’t hard mathematically, but if the function \\(Y=u(X)\\)is very complex or if \\(X\\) contains many parameters that are hard to estimate, simulation becomes much easier than precise calculations. That’s where simulation will tie in to our real estate valuation project."
  },
  {
    "objectID": "posts/real_estate_0/index.html#the-plan",
    "href": "posts/real_estate_0/index.html#the-plan",
    "title": "Real Estate Valuation with R: Introduction",
    "section": "The plan",
    "text": "The plan\nIn the next few posts, I’d like to introduce my real estate project and the very basic financial model behind it. Then, I’d like to expand that model by using Monte Carlo simulation to give better overall estimates for the value of the building. After that’s complete, I’d like to apply simple optimization to the project to see if we can estimate, with known parameters, what plan for the building would prove to make the most money. I don’t know much about real estate, but the project itself is simple enough to model, complicated enough to require advanced technical approaches, and applicable enough to the real world that the modeling will require me to explicitly scope the problems, goals, constraints, and implications of whatever decision is made.\nUp next: random distributions."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome!",
    "section": "",
    "text": "I accidentally started a career in data analytics by taking an Americorps VISTA position with Habitat for Humanity International in Atlanta. I worked data jobs with Atlanta nonprofits for two years. I self-taught a lot of R and SQL. Here’s my obligatory iris graphic, by the way.\n\nWith a stagnant job market and nothing but free time on my hands in late 2020, I applied to graduate programs and started studying at Carnegie Mellon University for a Master of Science Public Policy and Management. There, I’ve continued to learn a lot about R, Python, SQL, visualization tools, dashboard software, and data mining approaches.\nI hope to use this space to share things I’m trying to learn, my appreciation of R, and my love-hate relationship with Excel. Cheers!\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/chess_0_ggplot/index.html",
    "href": "posts/chess_0_ggplot/index.html",
    "title": "Learning about ggplot with chess",
    "section": "",
    "text": "I’m working on an assignment for a course on R Shiny. We have to explore any free, publicly available data and create an interactive dashboard with some visualizations and table displays. I am taking the opportunity to play with data about chess, a game I love to be bad at."
  },
  {
    "objectID": "posts/chess_0_ggplot/index.html#the-data",
    "href": "posts/chess_0_ggplot/index.html#the-data",
    "title": "Learning about ggplot with chess",
    "section": "The data",
    "text": "The data\nI have opted to use the International Chess Federation’s open source data on all chess players and their ratings, available here. The data is quite large, with nearly 1.2 million players represented, though many registered players have null values for their ratings.\nI wrote a quick cleaning script to remove some unwanted columns and to filter out players with null values. In particular, I filter out any player who has a standard rating of 0 or NULL. Check out repository sourcing this blog here for more details.\n\n\n\nThe data is pretty simple. I removed a few columns, then I made the Bdecade and Age columns by subtracting Byear from 2023 (so some people might be off by a year). Higher rated players have ratings in all three categories, but most of the data had a high degree of missingness. Of the nearly 1.2 million players represented in the data, only 397,040 players have a rating above zero in Standard time control. Filtering for SRtng > 0 may have been too simplistic - it’s possible there exist some people who have ratings in Rapid and Bullet but not standard - but I am mostly interested in displaying Standard ratings anyways.\nLet’s look at the highest rated players in Standard time control.\n\nratings |> # I'm trying to use the new pipes now\n  arrange(desc(SRtng)) |> \n  head(10) |> \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID Number\nName\nFed\nSex\nTit\nWTit\nSRtng\nRRtng\nBRtng\nByear\nBdecade\nAge\n\n\n\n\n1503014\nCarlsen, Magnus\nNOR\nM\nGM\nNA\n2859\n2839\n2852\n1990\n1990\n33\n\n\n4100018\nKasparov, Garry\nRUS\nM\nGM\nNA\n2812\n2783\n2712\n1963\n1960\n60\n\n\n8603677\nDing, Liren\nCHN\nM\nGM\nNA\n2811\n2829\n2787\n1992\n1990\n31\n\n\n4168119\nNepomniachtchi, Ian\nRUS\nM\nGM\nNA\n2793\n2761\n2781\n1990\n1990\n33\n\n\n12573981\nFirouzja, Alireza\nFRA\nM\nGM\nNA\n2785\n2745\n2904\n2003\n2000\n20\n\n\n2016192\nNakamura, Hikaru\nUSA\nM\nGM\nNA\n2768\n2750\n2879\n1987\n1980\n36\n\n\n2020009\nCaruana, Fabiano\nUSA\nM\nGM\nNA\n2766\n2758\n2818\n1992\n1990\n31\n\n\n24116068\nGiri, Anish\nNED\nM\nGM\nNA\n2764\n2714\n2807\n1994\n1990\n29\n\n\n5202213\nSo, Wesley\nUSA\nM\nGM\nNA\n2760\n2780\n2739\n1993\n1990\n30\n\n\n5000017\nAnand, Viswanathan\nIND\nM\nGM\nNA\n2754\n2731\n2733\n1969\n1960\n54\n\n\n\n\n\nWeird to think that some of these players also stream on Twitch."
  },
  {
    "objectID": "posts/chess_0_ggplot/index.html#a-note-on-ratings",
    "href": "posts/chess_0_ggplot/index.html#a-note-on-ratings",
    "title": "Learning about ggplot with chess",
    "section": "A note on ratings",
    "text": "A note on ratings\nChess uses a rating system called the Elo system to determine a player’s rating. There is some complicated math behind the rating that I don’t quite understand, but the difference in two players’ ratings is supposed to give us an insight into the probability of the outcomes for both players. Elo is used beyond chess: variations of it are used in everything from video games to dating apps.\nChess splits time controls into three categories: Standard, Rapid, and Bullet. You can play Blitz on chess.com or lichess.org but I’m not sure there are official ratings for 60 second games. In this data, SRtng, RRtng, and BRtng refer to Standard, Rapid, and Bullet."
  },
  {
    "objectID": "posts/chess_0_ggplot/index.html#distribution-visualizations",
    "href": "posts/chess_0_ggplot/index.html#distribution-visualizations",
    "title": "Learning about ggplot with chess",
    "section": "Distribution visualizations",
    "text": "Distribution visualizations\nLets look at the overall distribution of ratings. For this and all future visualizations, feel free to expand the code cells if you’re curious about plot construction with ggplot().\n\n\nCode\nlibrary(ggplot2)\n\nratings |> \n  ggplot(aes(x = SRtng)) +\n  geom_histogram(bins = 50) +\n  scale_fill_viridis_d() +\n  scale_y_continuous(label = scales::comma) +\n  xlab(NULL) +\n  ylab(NULL) +\n  labs(\n    title = 'Standard Elo Ratings',\n    caption = 'Data: FIDE, January 2023\\nViz: @bristowrichards'\n  ) +\n  theme_classic()\n\n\n\n\n\nChess Elo ratings are right-skewed with a range of roughly 1000 to around 2800. The distribution isn’t very normal: it has two distinct “elbows” around 1200 and again at around 1700. It looks like what could the combination of two underlying distributions, one of high skilled players and one of low skilled players.\n\nBy the decades\nI was curious about grouping this histogram by birth decade to see the relative size and ratings of different generations of players.\n\n\nCode\nratings |> \n  ggplot(aes(x = SRtng, fill = Bdecade)) +\n  geom_histogram(bins = 50) +\n  scale_fill_viridis_d() +\n  scale_y_continuous(label = scales::comma) +\n  xlab(NULL) +\n  ylab(NULL) +\n  labs(\n    title = 'Standard Elo Ratings, Frequency',\n    caption = 'Data: FIDE, January 2023\\nViz: @bristowrichards',\n    fill = 'Decade born'\n  ) +\n  theme_classic()\n\n\n\n\n\nThis is fascinating! There seems to be a massive number of players born between 2000 and 2010 that are mostly rated on the lower side of things. Each of the decade bands from before the year 2000 seem to have similar distributions, but a stacked histogram is a bad way to make visual assessments. Let’s use geom_freqpoly() to make frequency lines for each decade group so we can compare their frequencies at different ratings.\n\n\nCode\nratings |> \n  ggplot(aes(x = SRtng, color = Bdecade)) +\n  geom_freqpoly(bins = 50, linewidth = 1) +\n  scale_color_viridis_d() +\n  scale_y_continuous(label = scales::comma) +\n  xlab(NULL) +\n  ylab(NULL) +\n  labs(\n    title = 'Standard Elo Ratings, Frequency',\n    caption = 'Data: FIDE, January 2023\\nViz: @bristowrichards',\n    color = 'Decade born'\n  ) +\n  theme_classic()\n\n\n\n\n\nMy first impulse wasn’t very far off: the groups born before 2000 have pretty similar distributions, especially those born between 1960-1989. I’m fascinated by the apparent convergence into the shape of the 1980 generation. Will those born in the ’90s eventually approach the same distribution shape? What about those born in the early 2000s?\nAnd speaking of the early 2000s: look at that explosion in population! Those are people aged 14-24. That group contains Alireza Firouzja, the Persian prodigy who is the youngest player ever to reach 2800 Elo. It’s really cool to see in data that the population of players is growing in size and perhaps also in skill.\nIf each group has a different number of members, it can be hard to vizually compare age groups and their relative skills to other groups. To address this, we can use the geom_density() geom to plot the distributions as densities and not frequencies:\n\n\nCode\nratings |> \n  ggplot(aes(x = SRtng)) +\n  geom_density(aes(color = Bdecade), linewidth = 1) +\n  scale_color_viridis_d() +\n  geom_density(linetype = 'dashed', color = 'red', linewidth = 1.5) +\n  scale_y_continuous(label = scales::comma) +\n  xlab(NULL) +\n  ylab(NULL) +\n  labs(\n    title = 'Standard Elo Ratings, Density',\n    subtitle = 'The dashed line indicates all players',\n    caption = 'Data: FIDE, January 2023\\nViz: @bristowrichards',\n    color = 'Decade born'\n  ) +\n  theme_classic()\n\n\n\n\n\nOkay, now we can confirm that, at least visually, the generations born before the year 2000 seem to be converging to a similar general pattern, whereas younger players are much less skilled. Running some t tests comparing different age groups could be a fun statistics refresher, but I’ll save that for another post.\n\n\nComparing countries\nThe three highest-rated federations according to FIDE are the United States of America, Russia, and India, in that order. FIDE reports this list here by averaging the rating of the top ten players in each federation. Drawing a separate line for each of the 198 countries would be messy, but it should be easy to just select three countries. I’ll use dplyr::filter() before passing the data to the same ggplot calls as above.\n\n\nCode\nmy_countries <- c('USA', 'RUS', 'IND')\n\nratings |> \n  filter(is.element(Fed, my_countries)) |> \n  ggplot(aes(x = SRtng, fill = Fed)) +\n  geom_histogram(bins = 50) +\n  scale_fill_viridis_d(end = 0.6) +\n  scale_y_continuous(label = scales::comma) +\n  xlab(NULL) +\n  ylab(NULL) +\n  labs(\n    title = 'Standard Elo Ratings, Frequency',\n    caption = 'Data: FIDE, January 2023\\nViz: @bristowrichards',\n    fill = 'Federation'\n  ) +\n  theme_classic()\n\n\n\n\n\nIt is much more apparent that stacked histograms are inappropriate for comparing groups with this plot. Lets check geom_freqpoly() again:\n\n\nCode\nratings |> \n  filter(is.element(Fed, my_countries)) |> \n  ggplot(aes(x = SRtng, color = Fed)) +\n  geom_freqpoly(bins = 50, linewidth = 2) +\n  scale_color_viridis_d(end = 0.6) +\n  scale_y_continuous(label = scales::comma) +\n  xlab(NULL) +\n  ylab(NULL) +\n  labs(\n    title = 'Standard Elo Ratings, Frequency',\n    caption = 'Data: FIDE, January 2023\\nViz: @bristowrichards',\n    color = 'Federation'\n  ) +\n  theme_classic()\n\n\n\n\n\nThe big takeaway here seems to be that Russia and India have many many more players than the US has. We can scale back down with geom_density() to compare between groups, like before:\n\n\nCode\nratings |> \n  filter(is.element(Fed, my_countries)) |> \n  ggplot(aes(x = SRtng, color = Fed)) +\n  geom_density(size = 2) +\n  scale_color_viridis_d(end = 0.6) +\n  scale_y_continuous(label = scales::comma) +\n  xlab(NULL) +\n  ylab(NULL) +\n  labs(\n    title = 'Standard Elo Ratings, Density',\n    caption = 'Data: FIDE, January 2023\\nViz: @bristowrichards',\n    color = 'Federation'\n  ) +\n  theme_classic()\n\n\n\n\n\nFrom this distribution, it seems that the USA has a more generally skilled group of players. However, another interpretation that might perhaps be truer to life is that India is better at welcoming newer, younger players to the sport."
  },
  {
    "objectID": "posts/chess_0_ggplot/index.html#ggplot2-mechanics",
    "href": "posts/chess_0_ggplot/index.html#ggplot2-mechanics",
    "title": "Learning about ggplot with chess",
    "section": "ggplot2 mechanics",
    "text": "ggplot2 mechanics\nI am working on a dashboard project using this data, and I’d like to be able to change the type of plot displayed between these three kinds of distribution plots: geom_histogram(), geom_freqpoly(), and geom_density(). The good news is that these geoms all behave pretty similarly. So I’d like to make something like this:\n\n# theoretical input\nuser_input <- 'histogram'\n\n# theoretical plot code\nratings |> \n  ggplot(aes(x = SRtng, fill = Bdecade, color = Bdecade)) +\n  if (user_input == 'histogram') {\n    geom_histogram() +\n    labs(title = 'Histogram plot')\n  } else if (user_input == 'freqpoly') {\n    geom_freqpoly() +\n    labs(title = 'Frequency Polygon Plot')\n  }\n\nError:\n! Cannot add <ggproto> objects together\nℹ Did you forget to add this object to a <ggplot> object?\n\n\nWhen I ran code like this, I hit an error I had never seen previous to this week. I never knew why the + operator was used after ggplot() calls. The warning above led to some Google searching and I figured out why: everything after the ggplot() call is a ggproto object that provides a sort of list of options. A geom_histogram() provides a list of generic options unless I specify argument in the function call. A style_classic() function call provides a list of other options. These two lists of options have no meaningful way to interface with each other. So, if I nest two ggproto objects together in an if statement, added with a + sign, R will try to evaluate the meaning of those two objects added together before passing the combined result to the ggplot object. This is a problem! I couldn’t find out what to do and feared I would have to copy and paste a lot of code dependent on many different types of inputs.\nThankfully, Hadley Wickham and the other kind folks at Posit designed ggplot2 well. This issue of trying to bundle multiple ggproto objects together and pass them to the main ggplot() call seems like an important feature, but my approach above was not right. You don’t need to add ggproto objects together, you just have to combine them into a list. Using ggplot() + list(...), you can pull every non-null item out of any list and add it to your plot object.\n\nset.seed(1234) # to draw a reproducible sample\nratings |> \n  slice_sample(n = 1000) |> \n  ggplot(aes(x = Age, y = SRtng, color = Sex)) +\n  list(\n    geom_point(),\n    labs(\n      title = 'Age vs Standard Rating',\n      caption = 'Data: FIDE, January 2023\\nViz: @bristowrichards'\n    ),\n    scale_color_viridis_d(end = 0.6)\n  ) +\n  theme_classic()\n\n\n\n\nMost exciting of all, you can do this for nested lists! Look at the nested lists below. I added some NULL values to the list to show how ggplot2 recursively searches for all non-null values in a list to add to the main ggplot() call.\n\nratings |> \n  slice_sample(n = 1000) |> \n  ggplot(aes(x = Age, y = SRtng, color = Sex)) +\n  # first list\n  list(\n    geom_point(),\n    # first nested list\n    list(\n      labs(\n        title = 'Age vs Standard Rating',\n        caption = 'Data: FIDE, January 2023\\nViz: @bristowrichards'\n      ),\n      # ggplot will ignore this!\n      NULL\n    ),\n    scale_color_viridis_d(end = 0.6)\n  ) +\n  # I mean, come on\n  list(list(list(list(theme_classic())), NULL))\n\n\n\n\nThis bodes very well for conditionally creating different kinds of plots based on a user’s inputs. For now, I am planning on adding generic settings to one list, then creating a second list containing some different kinds of charts. Then, I plan to add both lists to one main ggplot() call. Alongside understanding how to recursively search through lists, ggplot2 knows to ignore any settings you manually set to NULL. This means I can build following this scheme: create a function that accepts user input, selectively paste ggproto elements into a list based on that input, then attach that list to the ggplot() call. Below I define such a function:\n\n# a theoretical list of inputs (shiny works like this)\nuser_input <- list(\n  plot_type = 'hist',\n  group = 'none'\n)\n\ncreate_plot <- function(input) {\n  # define generic settings to add to each plot regardless of inputs\n  plot_settings <- list(\n    labs(caption = 'Data: FIDE, January 2023\\nViz: @bristowrichards'),\n    theme_classic()\n  )\n  \n  # make list of different plots (this just has one as an example)\n  plot_list <- list(\n    'hist' = list(\n      geom_histogram(\n        aes(\n          fill = if (user_input$group != 'none') .data[[user_input$group]]\n        )\n      ),\n      labs(title = 'Histogram: Rapid Elo')\n    )\n  )\n  \n  # now combine and render everything\n  ratings |> \n    ggplot(aes(x = RRtng)) +\n    list(plot_settings, plot_list[[user_input$plot_type]])\n}\n\ncreate_plot(user_input)\n\n\n\n\nAbove, we set up the function to refer to the user input to find the desired plot type. We also defined the plot_list$hist to contain a histogram that could, if necessary, accept a group argument to give to the fill aesthetic. Let’s change the user’s input to make fill = Bdecade:\n\n# change input\nuser_input$group <- 'Bdecade'\n\ncreate_plot(user_input)\n\n\n\n\nIt works, kind of! I’m leaving the funny legend label because I think it’s insightful to see how ggplot2 is thinking about the argument I gave that fill aesthetic. Still, this is really insightful."
  },
  {
    "objectID": "posts/chess_0_ggplot/index.html#conclusion",
    "href": "posts/chess_0_ggplot/index.html#conclusion",
    "title": "Learning about ggplot with chess",
    "section": "Conclusion",
    "text": "Conclusion\nYou can make plots using lists of lists (of lists (of lists)) as long as every item at every level of that list is either a valid ggproto object or NULL. That means you can conditionally define things to become NULL when not needed, then attach a big list to whatever you need to render in the moment. I find this extremely helpful. This isn’t the biggest discovery in the world, but I am still proud to have tinkered with this long enough to learn some of the subtleties of the design of ggplot2."
  },
  {
    "objectID": "posts/chess_0_ggplot/index.html#further-reading",
    "href": "posts/chess_0_ggplot/index.html#further-reading",
    "title": "Learning about ggplot with chess",
    "section": "Further reading",
    "text": "Further reading\nThe link to download FIDE ratings is here.\nRead up on the very confusing math of competitive matchmaking Elo scores here.\nFor some fun reading about a dynamic sibling duo thriving at the intersection of chess and entertainment, read up on the Botez sisters.\nFor more about Alireza Firouzja, a prodigious young player and the youngest ever to reach 2800, read here. His story is fascinating and reveals a complicated relationship between the simple game of chess and the international politics undergirding the sport’s highest levels of competitive play.\nAll code powering this post and this site are hosted on my GitHub here."
  }
]